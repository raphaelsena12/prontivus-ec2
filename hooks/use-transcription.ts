"use client";

import { useState, useRef, useEffect, useCallback } from "react";
import { toast } from "sonner";
import { io, Socket } from "socket.io-client";

export interface TranscriptionEntry {
  time: string;
  speaker: string;
  text: string;
  isPartial?: boolean;
  speakerLabel?: string; // Label do AWS (spk_0, spk_1, etc.) ou identifica√ß√£o autom√°tica
}

export function useTranscription() {
  const [isTranscribing, setIsTranscribing] = useState(false);
  const [isPaused, setIsPaused] = useState(false);
  const [transcription, setTranscription] = useState<TranscriptionEntry[]>([]);
  const [currentPartial, setCurrentPartial] = useState<string>("");
  const [useAWSTranscribe, setUseAWSTranscribe] = useState(false);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const streamRef = useRef<MediaStream | null>(null);
  const recognitionRef = useRef<any>(null);
  const intervalRef = useRef<NodeJS.Timeout | null>(null);
  const socketRef = useRef<Socket | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const processorRef = useRef<ScriptProcessorNode | null>(null);
  const isStartingRef = useRef<boolean>(false);
  const restartTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  const recognitionStartTimeRef = useRef<number>(0);
  
  // Contador para alternar entre M√©dico e Paciente quando n√£o h√° identifica√ß√£o autom√°tica
  const speakerAlternatorRef = useRef<{ lastSpeaker: string; count: number }>({
    lastSpeaker: "M√©dico",
    count: 0,
  });

  // Fun√ß√£o para formatar tempo
  const formatTime = (date: Date) => {
    return date.toLocaleTimeString("pt-BR", {
      hour: "2-digit",
      minute: "2-digit",
      second: "2-digit",
    });
  };

  // Fun√ß√£o para determinar o speaker (com l√≥gica de altern√¢ncia quando n√£o h√° identifica√ß√£o autom√°tica)
  const determineSpeaker = useCallback((awsSpeakerLabel?: string, awsSpeaker?: string, currentTranscription?: TranscriptionEntry[]): string => {
    // Se o AWS identificou o speaker, usar
    if (awsSpeaker) {
      return awsSpeaker;
    }

    // Se h√° um label do AWS, mapear
    if (awsSpeakerLabel) {
      // spk_0 = M√©dico, spk_1 = Paciente (padr√£o)
      return awsSpeakerLabel === "spk_0" || awsSpeakerLabel.includes("0") 
        ? "M√©dico" 
        : "Paciente";
    }

    // Fallback: alternar entre M√©dico e Paciente baseado na √∫ltima entrada
    // Esta √© uma heur√≠stica b√°sica - o AWS Transcribe faz isso melhor com speaker diarization
    const transcriptList = currentTranscription || transcription;
    const lastEntry = transcriptList[transcriptList.length - 1];
    if (lastEntry && !lastEntry.isPartial) {
      // Alternar baseado no √∫ltimo speaker n√£o-parcial
      return lastEntry.speaker === "M√©dico" ? "Paciente" : "M√©dico";
    }

    return "M√©dico"; // Default para primeira entrada
  }, [transcription]);

  // Fun√ß√£o para enviar chunk de √°udio para AWS Transcribe com speaker diarization
  const sendAudioChunk = useCallback(async (audioBlob: Blob) => {
    try {
      const formData = new FormData();
      formData.append("audio", audioBlob, "audio.webm");

      const response = await fetch("/api/medico/transcribe", {
        method: "POST",
        body: formData,
      });

      if (!response.ok) {
        throw new Error("Erro ao enviar √°udio para transcri√ß√£o");
      }

      const data = await response.json();
      if (data.transcript) {
        const speaker = determineSpeaker(data.speakerLabel, data.speaker);
        return {
          transcript: data.transcript,
          speaker,
          speakerLabel: data.speakerLabel,
        };
      }
    } catch (error) {
      console.error("Erro ao enviar chunk de √°udio:", error);
    }
    return null;
  }, [determineSpeaker]);

  // Usar Web Speech API como fallback/principal (funciona sem AWS)
  const startWebSpeechRecognition = useCallback(() => {
    // Verificar se o navegador suporta Web Speech API
    if (
      !("webkitSpeechRecognition" in window) &&
      !("SpeechRecognition" in window)
    ) {
      toast.error(
        "Seu navegador n√£o suporta reconhecimento de voz. Use Chrome ou Edge."
      );
      return false;
    }

    const SpeechRecognition =
      (window as any).SpeechRecognition ||
      (window as any).webkitSpeechRecognition;

    // Verificar se j√° existe uma inst√¢ncia rodando
    if (recognitionRef.current) {
      console.log("J√° existe uma inst√¢ncia de reconhecimento ativa");
      try {
        recognitionRef.current.stop();
      } catch (e) {
        // Ignorar erro se j√° estiver parado
      }
    }

    const recognition = new SpeechRecognition();
    recognition.continuous = true;
    recognition.interimResults = true;
    recognition.lang = "pt-BR";
    
    console.log("Criando nova inst√¢ncia de SpeechRecognition");

    recognition.onstart = () => {
      recognitionStartTimeRef.current = Date.now();
      console.log("Web Speech Recognition iniciado com sucesso");
      isStartingRef.current = false;
      setIsTranscribing(true);
      toast.success("Transcri√ß√£o iniciada");
    };

    recognition.onresult = (event: any) => {
      let interimTranscript = "";
      let finalTranscript = "";

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        if (event.results[i].isFinal) {
          finalTranscript += transcript + " ";
        } else {
          interimTranscript += transcript;
        }
      }

      // Atualizar transcri√ß√£o parcial
      if (interimTranscript) {
        setCurrentPartial(interimTranscript);
      }

      // Adicionar transcri√ß√£o final
      if (finalTranscript.trim()) {
        setTranscription((prev) => {
          // Remover a √∫ltima entrada se for parcial
          const filtered = prev.filter((entry) => !entry.isPartial);
          const speaker = determineSpeaker(undefined, undefined, filtered);
          return [
            ...filtered,
            {
              time: formatTime(new Date()),
              speaker,
              text: finalTranscript.trim(),
            },
          ];
        });
        setCurrentPartial("");
      }
    };

    recognition.onerror = (event: any) => {
      console.error("Erro no reconhecimento de voz:", event.error, event);
      if (event.error === "no-speech") {
        // N√£o fazer nada, apenas aguardar - n√£o encerrar
        console.log("Nenhuma fala detectada, aguardando...");
        return;
      }
      if (event.error === "not-allowed") {
        console.error("Permiss√£o de microfone negada");
        isStartingRef.current = false;
        setIsTranscribing(false);
        toast.error("Permiss√£o de microfone negada. Por favor, permita o acesso ao microfone nas configura√ß√µes do navegador.");
      } else if (event.error === "aborted") {
        // Reconhecimento foi abortado manualmente
        console.log("Reconhecimento abortado");
        return;
      } else {
        console.error(`Erro na transcri√ß√£o: ${event.error}`);
        // N√£o parar imediatamente para erros menores
        if (event.error === "network" || event.error === "service-not-allowed") {
          isStartingRef.current = false;
          setIsTranscribing(false);
          toast.error(`Erro na transcri√ß√£o: ${event.error}`);
        }
      }
    };

    recognition.onend = () => {
      const endTime = Date.now();
      const duration = recognitionStartTimeRef.current > 0 
        ? endTime - recognitionStartTimeRef.current 
        : 0;
      
      console.log("Reconhecimento terminou. Dura√ß√£o:", duration, "ms. isTranscribing:", isTranscribing, "isPaused:", isPaused);
      
      // Se terminou muito rapidamente (< 1 segundo), pode ser um erro
      // N√£o reiniciar automaticamente nesse caso
      if (duration < 1000 && recognitionStartTimeRef.current > 0) {
        console.warn("Reconhecimento terminou muito rapidamente, possivelmente erro. N√£o reiniciando automaticamente.");
        isStartingRef.current = false;
        setIsTranscribing(false);
        recognitionStartTimeRef.current = 0;
        return;
      }
      
      // Resetar o tempo de in√≠cio
      recognitionStartTimeRef.current = 0;
      
      // Limpar timeout anterior se existir
      if (restartTimeoutRef.current) {
        clearTimeout(restartTimeoutRef.current);
      }
      
      // Reiniciar automaticamente se ainda estiver transcrevendo e n√£o estiver pausado
      // Adicionar delay para evitar loop infinito
      if (isTranscribing && !isPaused && recognitionRef.current === recognition) {
        restartTimeoutRef.current = setTimeout(() => {
          try {
            console.log("Reiniciando reconhecimento ap√≥s delay...");
            if (recognitionRef.current && isTranscribing && !isPaused) {
              recognitionRef.current.start();
            }
          } catch (error: any) {
            console.error("Erro ao reiniciar reconhecimento:", error);
            // Se o erro for "already started", est√° ok
            if (!error.message?.includes("already") && !error.message?.includes("started")) {
              setIsTranscribing(false);
            }
          }
        }, 1000); // Aumentar delay para 1 segundo
      } else {
        console.log("N√£o reiniciando - isTranscribing:", isTranscribing, "isPaused:", isPaused, "recognition match:", recognitionRef.current === recognition);
      }
    };

    // Salvar refer√™ncia antes de iniciar
    recognitionRef.current = recognition;
    
    try {
      console.log("Iniciando recognition.start()...");
      recognition.start();
      console.log("recognition.start() chamado com sucesso");
      return true;
    } catch (error: any) {
      console.error("Erro ao chamar recognition.start():", error);
      // Se j√° estiver rodando, considerar como sucesso
      if (error.message?.includes("already") || error.message?.includes("started") || error.message?.includes("aborted")) {
        console.log("Reconhecimento j√° est√° ativo ou foi abortado, tentando novamente...");
        // Limpar e tentar novamente ap√≥s um pequeno delay
        setTimeout(() => {
          try {
            recognition.start();
            setIsTranscribing(true);
          } catch (e) {
            console.error("Erro ao reiniciar:", e);
          }
        }, 100);
        setIsTranscribing(true);
        return true;
      }
      toast.error(`Erro ao iniciar transcri√ß√£o: ${error.message || 'Erro desconhecido'}`);
      setIsTranscribing(false);
      return false;
    }
  }, [isTranscribing, isPaused, determineSpeaker]);

  // Iniciar transcri√ß√£o via WebSocket (AWS Transcribe)
  const startAWSTranscription = useCallback(async (stream: MediaStream) => {
    try {
      // Conectar ao Socket.IO
      const socket = io("/api/socket", {
        path: "/api/socket",
        transports: ["websocket"],
      });

      socketRef.current = socket;

      // Configurar handlers
      socket.on("connect", () => {
        console.log("‚úÖ Conectado ao servidor WebSocket");
        socket.emit("start-transcription");
      });

      socket.on("connect_error", (error) => {
        console.error("‚ùå Erro ao conectar ao WebSocket:", error);
        toast.error("Erro ao conectar ao servidor de transcri√ß√£o");
        setIsTranscribing(false);
      });

      socket.on("transcription-started", () => {
        console.log("‚úÖ Transcri√ß√£o iniciada via AWS");
        setIsTranscribing(true);
        setUseAWSTranscribe(true);
        toast.success("Transcri√ß√£o AWS iniciada com speaker diarization");
      });

      socket.on("transcription-result", (data: {
        transcript: string;
        isPartial: boolean;
        speaker: string;
        speakerLabel?: string;
      }) => {
        console.log("üìù Transcri√ß√£o recebida:", { 
          transcript: data.transcript.substring(0, 50), 
          isPartial: data.isPartial,
          speaker: data.speaker 
        });
        
        if (data.isPartial) {
          setCurrentPartial(data.transcript);
        } else {
          setTranscription((prev) => {
            const filtered = prev.filter((entry) => !entry.isPartial);
            return [
              ...filtered,
              {
                time: formatTime(new Date()),
                speaker: data.speaker,
                text: data.transcript.trim(),
                speakerLabel: data.speakerLabel,
              },
            ];
          });
          setCurrentPartial("");
        }
      });

      socket.on("transcription-error", (error: { message: string }) => {
        console.error("‚ùå Erro na transcri√ß√£o:", error);
        toast.error(`Erro na transcri√ß√£o: ${error.message}`);
        setIsTranscribing(false);
      });

      // Configurar captura de √°udio em tempo real
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)({
        sampleRate: 16000,
      });
      audioContextRef.current = audioContext;

      const source = audioContext.createMediaStreamSource(stream);
      const processor = audioContext.createScriptProcessor(4096, 1, 1);
      processorRef.current = processor;

      let audioChunkCount = 0;
      processor.onaudioprocess = (e) => {
        if (!isPaused && socket.connected) {
          const inputData = e.inputBuffer.getChannelData(0);
          // Converter Float32Array para Int16Array (PCM)
          const pcmData = new Int16Array(inputData.length);
          for (let i = 0; i < inputData.length; i++) {
            const s = Math.max(-1, Math.min(1, inputData[i]));
            pcmData[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
          }
          // Converter para base64 e enviar (sem usar Buffer no cliente)
          const uint8Array = new Uint8Array(pcmData.buffer);
          const base64 = btoa(String.fromCharCode(...uint8Array));
          socket.emit("audio-chunk", { chunk: base64 });
          
          audioChunkCount++;
          if (audioChunkCount % 100 === 0) {
            console.log(`üé§ Enviados ${audioChunkCount} chunks de √°udio para AWS`);
          }
        } else if (!socket.connected) {
          console.warn("‚ö†Ô∏è Socket n√£o conectado, n√£o enviando √°udio");
        }
      };

      source.connect(processor);
      processor.connect(audioContext.destination);

      return true;
    } catch (error: any) {
      console.error("Erro ao iniciar transcri√ß√£o AWS:", error);
      return false;
    }
  }, [isPaused]);

  // Iniciar transcri√ß√£o
  const startTranscription = useCallback(async () => {
    // Prevenir m√∫ltiplas chamadas simult√¢neas
    if (isStartingRef.current) {
      console.log("J√° est√° iniciando, ignorando chamada duplicada");
      return;
    }
    
    if (isTranscribing && !isPaused) {
      console.log("J√° est√° transcrevendo, ignorando");
      return;
    }

    try {
      isStartingRef.current = true;
      console.log("Iniciando transcri√ß√£o...");
      setIsTranscribing(true);
      
      // Solicitar permiss√£o de microfone
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 16000,
        },
      });

      console.log("Permiss√£o de microfone obtida");
      streamRef.current = stream;

      // Tentar usar AWS Transcribe via WebSocket primeiro (se dispon√≠vel)
      const awsAvailable = process.env.NEXT_PUBLIC_USE_AWS_TRANSCRIBE === "true";
      console.log("üîç Verificando disponibilidade AWS:", {
        awsAvailable,
        envVar: process.env.NEXT_PUBLIC_USE_AWS_TRANSCRIBE,
      });
      
      if (awsAvailable) {
        console.log("üöÄ Tentando iniciar transcri√ß√£o AWS...");
        try {
          const awsStarted = await startAWSTranscription(stream);
          console.log("‚úÖ AWS iniciado?", awsStarted);
          if (awsStarted) {
            isStartingRef.current = false;
            return; // AWS est√° funcionando
          } else {
            console.warn("‚ö†Ô∏è AWS n√£o iniciou, usando fallback");
          }
        } catch (error: any) {
          console.error("‚ùå Erro ao iniciar AWS:", error);
          toast.error(`Erro ao iniciar AWS: ${error.message}`);
          // Continuar para fallback
        }
      } else {
        console.log("‚ÑπÔ∏è AWS n√£o habilitado, usando Web Speech API");
      }

      // Fallback: usar Web Speech API
      console.log("Tentando usar Web Speech API...");
      const started = startWebSpeechRecognition();
      console.log("Web Speech iniciado?", started);

      if (started) {
        // Web Speech API iniciou com sucesso
        setIsTranscribing(true);
        isStartingRef.current = false;
        toast.success("Transcri√ß√£o iniciada");
      } else {
        // Fallback: usar MediaRecorder e enviar para AWS
        const mediaRecorder = new MediaRecorder(stream, {
          mimeType: "audio/webm;codecs=opus",
        });

        mediaRecorderRef.current = mediaRecorder;
        audioChunksRef.current = [];

        mediaRecorder.ondataavailable = (event) => {
          if (event.data.size > 0) {
            audioChunksRef.current.push(event.data);
          }
        };

        mediaRecorder.onstop = async () => {
          const audioBlob = new Blob(audioChunksRef.current, {
            type: "audio/webm",
          });
          const result = await sendAudioChunk(audioBlob);
          if (result) {
            setTranscription((prev) => [
              ...prev,
              {
                time: formatTime(new Date()),
                speaker: result.speaker,
                text: result.transcript,
                speakerLabel: result.speakerLabel,
              },
            ]);
          }
        };

        // Enviar chunks a cada 3 segundos
        console.log("Iniciando MediaRecorder...");
        mediaRecorder.start();
        setIsTranscribing(true);
        isStartingRef.current = false;
        toast.success("Transcri√ß√£o iniciada");

        intervalRef.current = setInterval(() => {
          if (mediaRecorder.state === "recording") {
            mediaRecorder.stop();
            mediaRecorder.start();
          }
        }, 3000);
      }
    } catch (error: any) {
      console.error("Erro ao iniciar transcri√ß√£o:", error);
      isStartingRef.current = false;
      toast.error(
        error.message || "Erro ao acessar o microfone. Verifique as permiss√µes."
      );
      setIsTranscribing(false);
    }
  }, [startWebSpeechRecognition, sendAudioChunk, startAWSTranscription, isTranscribing, isPaused]);

  // Pausar transcri√ß√£o
  const pauseTranscription = useCallback(() => {
    setIsPaused(true);

    // Pausar Web Speech API
    if (recognitionRef.current) {
      recognitionRef.current.stop();
    }

    // Pausar MediaRecorder
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === "recording") {
      mediaRecorderRef.current.pause();
    }

    // Pausar stream de √°udio (sem parar completamente)
    if (streamRef.current) {
      streamRef.current.getTracks().forEach((track) => {
        track.enabled = false;
      });
    }

    toast.info("Transcri√ß√£o pausada");
  }, []);

  // Retomar transcri√ß√£o
  const resumeTranscription = useCallback(() => {
    setIsPaused(false);

    // Retomar Web Speech API
    if (recognitionRef.current) {
      try {
        recognitionRef.current.start();
      } catch (error) {
        // Se j√° estiver rodando, ignorar erro
        console.log("Reconhecimento j√° est√° ativo");
      }
    }

    // Retomar MediaRecorder
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === "paused") {
      mediaRecorderRef.current.resume();
    }

    // Retomar stream de √°udio
    if (streamRef.current) {
      streamRef.current.getTracks().forEach((track) => {
        track.enabled = true;
      });
    }

    toast.success("Transcri√ß√£o retomada");
  }, []);

  // Parar transcri√ß√£o
  const stopTranscription = useCallback(() => {
    isStartingRef.current = false;
    
    // Limpar timeout de rein√≠cio
    if (restartTimeoutRef.current) {
      clearTimeout(restartTimeoutRef.current);
      restartTimeoutRef.current = null;
    }
    
    setIsTranscribing(false);
    setIsPaused(false);
    setUseAWSTranscribe(false);

    // Parar WebSocket/AWS
    if (socketRef.current) {
      socketRef.current.emit("stop-transcription");
      socketRef.current.disconnect();
      socketRef.current = null;
    }

    // Parar processamento de √°udio
    if (processorRef.current) {
      processorRef.current.disconnect();
      processorRef.current = null;
    }

    if (audioContextRef.current) {
      try {
        // Verificar se o AudioContext n√£o est√° j√° fechado
        if (audioContextRef.current.state !== 'closed') {
          audioContextRef.current.close();
        }
      } catch (error) {
        console.warn("Erro ao fechar AudioContext:", error);
      }
      audioContextRef.current = null;
    }

    // Parar Web Speech API
    if (recognitionRef.current) {
      recognitionRef.current.stop();
      recognitionRef.current = null;
    }

    // Parar MediaRecorder
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== "inactive") {
      mediaRecorderRef.current.stop();
      mediaRecorderRef.current = null;
    }

    // Limpar intervalo
    if (intervalRef.current) {
      clearInterval(intervalRef.current);
      intervalRef.current = null;
    }

    // Parar stream de √°udio
    if (streamRef.current) {
      streamRef.current.getTracks().forEach((track) => track.stop());
      streamRef.current = null;
    }

    // Converter √∫ltima entrada parcial em final antes de limpar
    setTranscription((prev) => {
      const filtered = prev.filter((entry) => !entry.isPartial);
      const partialEntries = prev.filter((entry) => entry.isPartial);
      
      // Se houver entradas parciais, converter a √∫ltima em final
      if (partialEntries.length > 0) {
        const lastPartial = partialEntries[partialEntries.length - 1];
        if (lastPartial.text && lastPartial.text.trim()) {
          return [
            ...filtered,
            {
              ...lastPartial,
              isPartial: false,
            },
          ];
        }
      }
      
      return filtered;
    });
    
    // Limpar transcri√ß√£o parcial
    setCurrentPartial("");
    toast.info("Transcri√ß√£o parada");
  }, []);

  // Limpar ao desmontar
  useEffect(() => {
    return () => {
      stopTranscription();
    };
  }, [stopTranscription]);

  // Atualizar transcri√ß√£o com entrada parcial
  useEffect(() => {
    if (currentPartial) {
      setTranscription((prev) => {
        // Remover entradas parciais anteriores
        const filtered = prev.filter((entry) => !entry.isPartial);
        return [
          ...filtered,
          {
            time: formatTime(new Date()),
            speaker: "M√©dico",
            text: currentPartial,
            isPartial: true,
          },
        ];
      });
    } else {
      // Remover entrada parcial se n√£o houver mais texto parcial
      setTranscription((prev) => prev.filter((entry) => !entry.isPartial));
    }
  }, [currentPartial]);

  // Processar transcri√ß√£o com IA
  const processTranscription = useCallback(async (
    aiModel: 'openai' = 'openai',
    examesIds?: string[]
  ): Promise<{
    anamnese: string;
    cidCodes: Array<{ code: string; description: string; score: number }>;
    exames: Array<{ nome: string; tipo: string; justificativa: string }>;
    prescricoes: Array<{ medicamento: string; dosagem: string; posologia: string; duracao: string; justificativa?: string }>;
  } | null> => {
    try {
      console.log("Processando transcri√ß√£o. Total de entradas:", transcription.length);
      console.log("Modelo de IA selecionado:", aiModel);
      console.log("Exames selecionados:", examesIds);
      console.log("Entradas:", transcription);
      
      // Primeiro, tentar usar apenas entradas n√£o parciais
      let transcriptionText = transcription
        .filter((entry) => !entry.isPartial)
        .map((entry) => entry.text)
        .join(" ");

      // Se n√£o houver entradas n√£o parciais, usar todas (incluindo parciais)
      if (!transcriptionText.trim() && transcription.length > 0) {
        console.log("Nenhuma entrada n√£o parcial encontrada. Usando todas as entradas...");
        transcriptionText = transcription
          .map((entry) => entry.text)
          .join(" ");
      }

      console.log("Texto da transcri√ß√£o para processar:", transcriptionText);
      console.log("Tamanho do texto:", transcriptionText.length);

      if (!transcriptionText.trim() && (!examesIds || examesIds.length === 0)) {
        console.error("Transcri√ß√£o vazia e nenhum exame selecionado");
        throw new Error("Nenhuma transcri√ß√£o dispon√≠vel para processar e nenhum exame selecionado");
      }

      const response = await fetch("/api/medico/process-transcription", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          transcription: transcriptionText,
          aiModel, // Passar o modelo selecionado
          examesIds: examesIds || [], // Passar IDs dos exames selecionados
        }),
      });

      if (!response.ok) {
        const error = await response.json();
        throw new Error(error.error || "Erro ao processar transcri√ß√£o");
      }

      const data = await response.json();
      return data.analysis;
    } catch (error: any) {
      console.error("Erro ao processar transcri√ß√£o:", error);
      throw error;
    }
  }, [transcription]);

  return {
    isTranscribing,
    isPaused,
    transcription,
    startTranscription,
    pauseTranscription,
    resumeTranscription,
    stopTranscription,
    processTranscription,
  };
}

